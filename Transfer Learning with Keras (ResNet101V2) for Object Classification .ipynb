{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transfer Learning with Keras (ResNet101V2) for Object Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pathlib\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "img_height, img_width, img_channel = 224, 224, 3\n",
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "initial_learning_rate = 0.001\n",
    "classes = 5\n",
    "\n",
    "# mean and std normalization values for ImageNet from https://discuss.pytorch.org/t/discussion-why-normalise-according-to-imagenet-mean-and-std-dev-for-transfer-learning/115670\n",
    "MEAN = 255 * np.array([0.485, 0.456, 0.406]) \n",
    "STD = 255 * np.array([0.229, 0.224, 0.225]) \n",
    "\n",
    "# Import dataset, this is an example dataset containing 5 different classe of flower photos\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
    "# data_dir = pathlib.Path(data_dir) # we can devide the images into training and test folders\n",
    "data_dir = pathlib.Path('data/dataset_train/') # 80% training, 10% validation\n",
    "test_dir = pathlib.Path('data/dataset_test/') # 10% testing\n",
    "validation_split = 0.1\n",
    "n_seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split = validation_split,\n",
    "  subset=\"training\",\n",
    "  seed = n_seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "  \n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split = validation_split,\n",
    "  subset=\"validation\",\n",
    "  seed = n_seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "class_names = train_ds.class_names   # classes \n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data \n",
    "def process(image,label):\n",
    "    image = tf.cast((image - MEAN) / STD ,tf.float32)\n",
    "    return image,label\n",
    "\n",
    "train_ds = train_ds.map(process)\n",
    "val_ds = val_ds.map(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our data\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(25):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ResNet101V2 Pre-trained Model\n",
    "# you can choose other pretrained models from here https://keras.io/api/applications/\n",
    "model = Sequential()\n",
    "\n",
    "pretrained_model= tf.keras.applications.ResNet101V2(include_top=False,\n",
    "                   input_shape = (img_height, img_width, img_channel), \n",
    "                   pooling = 'avg', classes = classes,\n",
    "                   weights = 'imagenet')\n",
    "for layer in pretrained_model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "model.add(pretrained_model)\n",
    "\n",
    "# Adding a fully connected and output layer where actual learning can take place\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential decay schedule: learning rate between epochs or iterations as the training progresses\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "# Model compiler with respect to Learning Rate Scheduler\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compiler without Exponential decay schedule\n",
    "model.compile(optimizer = Adam(learning_rate = initial_learning_rate),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training (model fit)\n",
    "# ReduceLROnPlateau: Reduce learning rate  \n",
    "tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"accuracy\",\n",
    "    factor=0.1,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0\n",
    ")\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "history = model.fit(train_ds, validation_data=val_ds, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training (model fit)\n",
    "# Weighted loss class imbalance\n",
    "class_weight = {0: 633, \n",
    "                1: 898, \n",
    "                2: 641, \n",
    "                3: 699, \n",
    "                4: 799}\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs= n_epochs, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "fig1 = plt.gcf()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.axis(ymin=0.4,ymax=1)\n",
    "plt.grid()\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.grid()\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Inference\n",
    "predicted_class_indices = [] # initializing an array to save prediction values\n",
    "GT_class_indices = [] # initializing an array to keep the ground trurth values\n",
    "\n",
    "for folder in os.listdir(test_dir):\n",
    "    for img in os.listdir(str(test_dir) + '/' + str(folder)):\n",
    "        image = cv2.imread(str(test_dir) + '/' + str(folder) + '/' + img)\n",
    "        # Normalizing data\n",
    "        # Note: next two lines should be uncommented in case of skipping normalzing the training data\n",
    "        image = np.array(image)\n",
    "        image = (image - MEAN) / STD\n",
    "        \n",
    "        image_resized = cv2.resize(image, (img_height,img_width)) # resize the image to the learnt size in the training\n",
    "        image = np.expand_dims(image_resized,axis=0) # expanding the image dimesion to 4D  \n",
    "        \n",
    "        # Prediction on testing data\n",
    "        pred = model.predict(image)\n",
    "        \n",
    "        output_class = class_names[np.argmax(pred)] \n",
    "        predicted_class_indices.append(output_class) # prediction array\n",
    "        GT_class_indices.append(folder) # ground truth array\n",
    "        \n",
    "        print(\"The GT is\", str(folder),\" and the predicted class is\", output_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for evaluation of the model\n",
    "cm = confusion_matrix(predicted_class_indices, GT_class_indices)\n",
    "cm_df = pd.DataFrame(cm)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize = (5,4))\n",
    "sns.heatmap(cm_df, annot = True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total recall and precision for evaluation of the model\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "recall = np.mean(recall)\n",
    "precision = np.mean(precision)\n",
    "print(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving prediction vs ground truth in a csv file\n",
    "results=pd.DataFrame({\"Filename\":GT_class_indices,\n",
    "                      \"Predictions\":predicted_class_indices})\n",
    "results.to_csv(str(test_dir) + \"results.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
